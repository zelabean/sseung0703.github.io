---
layout: post
title: "2020_3rd_quarter"
date: 2020-07-02
image_url: https://user-images.githubusercontent.com/32321592/91457593-79517300-e8bf-11ea-865b-57d44c1464ac.PNG
mathjax: true
comments: true
---

# Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks
## Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, Ping Wang, NeurIPS 2019
### 발표자: 이승현 [[paper link](https://arxiv.org/abs/1909.08174), [presentation material](https://trello-attachments.s3.amazonaws.com/5d15b7297b29f54b88064f86/5f3a18b1378bf75d14551518/d9d8f8016b1e58e1877fc6a43b1cd812/GDP.pdf)]
- Filter pruning 기법 중 하나로 기존 기법에 비해 효과적으로 filter의 importance를 정의하고, 이를 기반으로 filter를 제거 및 pruned network를 tuning할 수 있는 방법을 제안한 논문입니다.
- Gate decorator란 Taylor expansion에 기반한 기법으로, pruning할 각 layer에 gate vector를 추가 및 이의 gradient에 기반한 score를 정하며 이는 아래와 같습니다.

$$\Theta(\phi_i) = \sum_{(X, Y) \in \mathcal D}\left| \frac{\delta \mathcal{L}(X,Y;\theta)}{\delta \phi_i} \phi_i \right| $$

- Tick step에서는 gate를 tuning 및 일정 비율을 pruning하며, Tock step에서는 손실된 성능을 복원하기 위해 일정 epoch만큼 fine-tuning 합니다. 이 때 Tock step에서는 gate에 $$L_1$$-regularization을 가해 sparce해지도록 만듭니다.
- Pruning 시에 network의 특성에 따라 서로 종속적인 layer들을 group으로 묶어 하나의 gate를 통해 importance score를 계산 및 동시에 pruning되도록 합니다.
- 높은 성능을 보이며 몇 가지 중요한 insight를 주는 좋은 기법으로 보이지만, pruning 시간이 매우 길다는 것은 큰 단점으로 보입니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/26036843/90788752-14d36880-e341-11ea-829e-2485ce943e91.png">
  <img src="https://user-images.githubusercontent.com/26036843/90788498-d63dae00-e340-11ea-87b5-7a0f37c1d93a.png">
</p>


# Data-Efficient Hierarchical Reinforcement Learning (HIRO)
## Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine
### 발표자: 주동욱 [[paper link](https://arxiv.org/abs/1805.08296), [presentation material](https://trello-attachments.s3.amazonaws.com/5d15b7297b29f54b88064f86/5f433e7cd7804e0c251f6146/0f5d11f0886ebead360e5b1119f652ea/HIRO_%EC%A3%BC%EB%8F%99%EC%9A%B1.pdf)]
- Task Hierarchy가 복잡한 Continuous Action Space 환경에서 Generally Applicable Off-Policy HRL 알고리즘을 제안하였습니다.
- 기존의 HRL은 higher-level policy를 최적화하는 과정에서 non-stationary problem으로 인하여 On-Policy 방식으로 업데이트를 하여 Sample Efficiency가 떨어지는 단점이 있었지만, 본 논문에서는 Off-Policy Correction을 통하여 이를 해결한 Off-Policy 알고리즘을 제안하였습니다.
- 단점으로는 HRL 이 모두 그렇지만 Task 의 Hierarchical Structure 가 명확하지 않을 때는 쓰기 어렵습니다. 반대로 Locomotive Task 계열에서는 자연스럽게 적용 가능할 것으로 보입니다.
- General HRL Algorithm 중에서 비교적 초기의(2018) 논문이므로 이어질 이후 이어지는 연구 동향을 살펴보면 좋을 것 같습니다.

<p align="center">
  <img src="https://user-images.githubusercontent.com/32321592/91457593-79517300-e8bf-11ea-865b-57d44c1464ac.PNG">
  <img src="https://user-images.githubusercontent.com/32321592/91457597-7a82a000-e8bf-11ea-9d1c-aabe88d567ad.PNG">
</p>

# wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations
## Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli
### 발표자: 김성빈

- Self-supervised 방식의 음성 representation 학습 기법으로, 많은 시간의 unlabeled 음성을 pretrain을 한 후에 단 10분의 labeled 음성만으로도 사용 가능한 수준의 음성인식 성능을 낼 수 있음
- 기존 vq-wav2vec은 continous한 음성을 discrete한 토큰으로 바꿔준 후, Bert 학습을 통해 Masking prediction을 하는 Two-step으로 구성되어있음
- quantization 및 Two-step으로 인한 성능저하를 이 논문에서는 Continous input을 사용하여 End-to-End로 학습하는 방식을 제안하여 해결
(작성중)
 



